{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo on how to use AWS Marpetplace Machine Learning Products with AWS Data Exchange products\n",
    "\n",
    "@Copyright Rick Cao\n",
    "\n",
    "This notebook serves a demo on using the below mentioned algorithm product and data products to create a model endpoint. \n",
    "\n",
    " - [Scikit Decision Trees](https://aws.amazon.com/marketplace/pp/prodview-ha4f3kqugba3u?qid=1591728221699&sr=0-1&ref_=srh_res_product_title)\n",
    " \n",
    " - [COVID-19 - World Confirmed Cases, Deaths, and Testing](https://aws.amazon.com/marketplace/pp/prodview-3b32sjummof5s?qid=1591728365105&sr=0-1&ref_=srh_res_product_title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import re \n",
    "import time\n",
    "import click\n",
    "import uuid\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = boto3.client('dataexchange', region_name='us-east-1')\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_revisions(data_set_id):\n",
    "\n",
    "    revisions = []\n",
    "    res = dx.list_data_set_revisions(DataSetId=data_set_id)\n",
    "    next_token = res.get('NextToken')\n",
    "    \n",
    "    revisions += res.get('Revisions')\n",
    "    while next_token:\n",
    "        res = dx.list_data_set_revisions(DataSetId=data_set_id,\n",
    "                                         NextToken=next_token)\n",
    "        revisions += res.get('Revisions')\n",
    "        next_token = res.get('NextToken')\n",
    "        \n",
    "    return revisions\n",
    "\n",
    "\n",
    "def get_all_assets(data_set_id, revision_id):\n",
    "    assets = []\n",
    "    res = dx.list_revision_assets(DataSetId=data_set_id,\n",
    "                                  RevisionId=revision_id)\n",
    "    next_token = res.get('NextToken')\n",
    "    \n",
    "    assets += res.get('Assets')\n",
    "    while next_token:\n",
    "        res = dx.list_revision_assets(DataSetId=data_set_id,\n",
    "                                      RevisionId=revision_id,\n",
    "                                      NextToken=next_token)\n",
    "        assets += res.get('Assets')\n",
    "        next_token = res.get('NextToken')\n",
    "        \n",
    "    return assets\n",
    "\n",
    "\n",
    "def get_entitled_data_sets():\n",
    "    data_sets = []\n",
    "    res = dx.list_data_sets(Origin='ENTITLED')\n",
    "    next_token = res.get('NextToken')\n",
    "    \n",
    "    data_sets += res.get('DataSets')\n",
    "    while next_token:\n",
    "        res = dx.list_data_sets(Origin='ENTITLED',\n",
    "                                NextToken=next_token)\n",
    "        data_sets += res.get('DataSets')\n",
    "        next_token = res.get('NextToken')\n",
    "        \n",
    "    return data_sets\n",
    "\n",
    "\n",
    "def export_assets(assets, bucket):\n",
    "    \n",
    "    asset_destinations = []\n",
    "\n",
    "    for asset in assets:\n",
    "        asset_destinations.append({\n",
    "            \"AssetId\": asset.get('Id'),\n",
    "            \"Bucket\": bucket,\n",
    "            \"Key\": asset.get('Name')\n",
    "        })\n",
    "\n",
    "    job = dx.create_job(Type='EXPORT_ASSETS_TO_S3', Details={\n",
    "        \"ExportAssetsToS3\": {\n",
    "            \"RevisionId\": asset.get(\"RevisionId\"), \"DataSetId\": asset.get(\"DataSetId\"),\n",
    "            \"AssetDestinations\": asset_destinations\n",
    "        }\n",
    "    })\n",
    "\n",
    "    job_id = job.get('Id')\n",
    "    dx.start_job(JobId=job_id)\n",
    "\n",
    "    while True:\n",
    "        job = dx.get_job(JobId=job_id)\n",
    "\n",
    "        if job.get('State') == 'COMPLETED':\n",
    "            break\n",
    "        elif job.get('State') == 'ERROR':\n",
    "            raise Exception(\"Job {} failed to complete - {}\".format(\n",
    "                job_id, job.get('Errors')[0].get('Message'))\n",
    "            )\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "def to_url(s):\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    s = re.sub(r\"\\s+\", '-', s)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def download_assets(assets, bucket, asset_dir):\n",
    "    for asset in assets:\n",
    "        asset_name = asset.get('Name')\n",
    "        sub_dir = os.path.dirname(asset_name)\n",
    "        full_dir = os.path.join(asset_dir, sub_dir)\n",
    "\n",
    "        if not os.path.exists(full_dir):\n",
    "            os.makedirs(full_dir)\n",
    "\n",
    "        asset_file = os.path.join(full_dir, os.path.basename(asset_name))\n",
    "\n",
    "        s3.download_file(bucket, asset_name, asset_file)\n",
    "\n",
    "        print(\"Downloaded file {}\".format(asset_file))\n",
    "\n",
    "\n",
    "def make_s3_staging_bucket():\n",
    "    bucket_name = str(uuid.uuid4())\n",
    "    s3.create_bucket(Bucket=bucket_name)\n",
    "    return bucket_name\n",
    "\n",
    "\n",
    "def remove_s3_bucket(bucket_name):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    bucket = s3_resource.Bucket(bucket_name)\n",
    "    bucket.objects.all().delete()\n",
    "    bucket.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "# S3 prefixes\n",
    "common_prefix = \"DEMO-scikit\"\n",
    "training_input_prefix = common_prefix + \"/training-input-data\"\n",
    "batch_inference_input_prefix = common_prefix + \"/batch-inference-input-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sage.Session()\n",
    "staging_bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sets = get_entitled_data_sets()\n",
    "len(data_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded file covid19/f340dca7434c4817a900c813fe1e5adc/covid-19-world-cases-deaths-testing/dataset/covid-19-world-cases-deaths-testing.csv\n",
      "Downloaded file covid19/f340dca7434c4817a900c813fe1e5adc/covid-19-world-cases-deaths-testing/dataset/covid-19-world-cases-deaths-testing.xlsx\n"
     ]
    }
   ],
   "source": [
    "ds_id = \"5d6ec0f777f3e82430693c096426adf1\" # rearc COVID-19 - World Confirmed Cases, Deaths, and Testing\n",
    "\n",
    "last_rev = get_all_revisions(ds_id)[0]\n",
    "assets = get_all_assets(ds_id, last_rev.get('Id'))\n",
    "\n",
    "destination_dir = os.path.join('covid19', last_rev.get('Id'))\n",
    "\n",
    "export_assets(assets, staging_bucket)\n",
    "download_assets(assets, staging_bucket, destination_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data from S3 bucket and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key = 'covid-19-world-cases-deaths-testing.csv'\n",
    "data_location = 's3://{}/{}/{}'.format(staging_bucket, \"covid-19-world-cases-deaths-testing/dataset\", data_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['date']==\"2020-06-08\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['iso_code', 'continent', 'location','date','total_cases','new_cases',\n",
    "      'total_deaths','new_deaths','total_cases_per_million','new_cases_per_million',\n",
    "      'total_deaths_per_million','new_deaths_per_million', 'stringency_index', 'population', \n",
    "      'population_density', 'median_age','aged_65_older', 'aged_70_older', 'gdp_per_capita',\n",
    "     'cvd_death_rate', 'diabetes_prevalence', 'handwashing_facilities', 'hospital_beds_per_thousand']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df[['total_cases','new_cases',\n",
    "      'total_deaths','new_deaths','total_cases_per_million','new_cases_per_million',\n",
    "           'new_deaths_per_million', 'stringency_index', 'population', \n",
    "      'population_density', 'median_age','aged_65_older', 'aged_70_older', 'gdp_per_capita',\n",
    "     'cvd_death_rate', 'diabetes_prevalence', 'handwashing_facilities', 'hospital_beds_per_thousand']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314.30049999999994"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['total_cases_per_million'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_cases_per_million'] = df['total_cases_per_million'] > 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_X.values , df['total_cases_per_million'].values, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.nan_to_num(X_train)\n",
    "X_val = np.nan_to_num(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.hstack((y_train.reshape(y_train.shape[0],1), X_train))\n",
    "# input_data = np.hstack((X_train, y_train.reshape(188,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(input_data).to_csv(\"input_data.csv\", index=False)\n",
    "training_input_prefix = \"training_input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Location s3://sagemaker-us-east-1-833255123925/training_input/input_data.csv\n"
     ]
    }
   ],
   "source": [
    "training_input = sagemaker_session.upload_data(\"input_data.csv\", key_prefix=training_input_prefix)\n",
    "print (\"Training Data Location \" + training_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the algorithm product and submit a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_arn = \"arn:aws:sagemaker:us-east-1:865070037744:algorithm/scikit-decision-trees-15423055-57b73412d2e93e9239e4e16f83298b8f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from sagemaker.algorithm import AlgorithmEstimator\n",
    "\n",
    "algo = AlgorithmEstimator(\n",
    "            algorithm_arn=algorithm_arn,\n",
    "            role=role,\n",
    "            train_instance_count=1,\n",
    "            train_instance_type='ml.m4.xlarge',\n",
    "            base_job_name='intel-from-aws-marketplace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-09 22:33:49 Starting - Starting the training job...\n",
      "2020-06-09 22:33:51 Starting - Launching requested ML instances......\n",
      "2020-06-09 22:35:08 Starting - Preparing the instances for training......\n",
      "2020-06-09 22:36:17 Downloading - Downloading input data\n",
      "2020-06-09 22:36:17 Training - Downloading the training image...\n",
      "2020-06-09 22:36:48 Training - Training image download completed. Training in progress..\u001b[34mStarting the training.\u001b[0m\n",
      "\u001b[34mvalidation-accuracy: 1.0\u001b[0m\n",
      "\u001b[34mTraining complete.\u001b[0m\n",
      "\n",
      "2020-06-09 22:37:00 Uploading - Uploading generated training model\n",
      "2020-06-09 22:37:00 Completed - Training job completed\n",
      "Training seconds: 61\n",
      "Billable seconds: 61\n"
     ]
    }
   ],
   "source": [
    "algo.fit({'training': training_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Model Tuning (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter\n",
    "\n",
    "## This demo algorithm supports max_leaf_nodes as the only tunable hyperparameter.\n",
    "hyperparameter_ranges = {'max_leaf_nodes': IntegerParameter(1, 10)}\n",
    "\n",
    "tuner = HyperparameterTuner(estimator=algo, base_tuning_job_name='some-name',\n",
    "                                objective_metric_name='validation:accuracy',\n",
    "                                hyperparameter_ranges=hyperparameter_ranges,\n",
    "                                max_jobs=2, max_parallel_jobs=2)\n",
    "\n",
    "tuner.fit({'training': training_input}, include_cls_metadata=False)\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the endpoint and test your prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n",
      "-------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "predictor = algo.deploy(1, 'ml.m4.xlarge', serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.predictor import RealTimePredictor\n",
    "# predictor1 = RealTimePredictor(endpoint=\"intel-from-aws-marketplace-2020-06-09-02-08-55-620\",\n",
    "#                               sagemaker_session=sagemaker_session,\n",
    "#                               serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor1.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORM_WORKDIR = \"data/transform\"\n",
    "# X_val.to_csv(TRANSFORM_WORKDIR + \"/batchtransform_test.csv\", index=False, header=False)\n",
    "\n",
    "# transform_input = sagemaker_session.upload_data(TRANSFORM_WORKDIR, key_prefix=batch_inference_input_prefix) + \"/batchtransform_test.csv\"\n",
    "# print(\"Transform input uploaded to \" + transform_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer = algo.transformer(1, 'ml.m4.xlarge')\n",
    "# transformer.transform(transform_input, content_type='text/csv')\n",
    "# transformer.wait()\n",
    "\n",
    "# print(\"Batch Transform output saved to \" + transformer.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictor.predict(X_val).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=np.array(prediction.split(\"\\n\")[:-1], dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 1., 1.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# print(accuracy_score(y_val, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We could do more powerful by using streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
